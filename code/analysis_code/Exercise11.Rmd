---
title: "Exercise 11"
author: "Nicholas Mallis"
date: "11/2/2021"
output: html_document
---


Note: The contents of this exercise (that work) include code, output, and figures for fitting the null models and Decision Tree. I unfortunately could not get my LASSO or Random Forest models to work, even after using your code. Everytime I try to run tune_grid(), R crashes. I tried multiple ways and seeked help, but nothing would work.I'm not sure if it's my computer. I have a seperate R script called LASSO and RF.R which includes include code, output, and figures for fitting the null models, LASSO, and Random Forests. While these do not work on my computer, maybe they will on yours?



# Setting up
```{r message=FALSE, results =FALSE}
#loading packages
library(broom)
library(here) #for data loading/saving
library(tidyverse)
library(recipes)
library(tidymodels)
library(workflowr) 
library(parsnip)
library(rsample)
library(rpart)
library(glmnet)
library(ranger)
library(modeldata)
library(rpart.plot)
library(dials)
library(workflows)
library(vip)
library(glmnet)
library(yardstick)
library(doParallel) # for parallel computing 
```


#Loading Data


```{r message=FALSE, results =FALSE}
#first loading in processed data
data_location <- here::here("data","processed_data","processeddata.rds")

#load data. 
data <- readRDS(data_location)

#checking
glimpse(data)

```

#Setting up: Random Seed, Data Split, and Cross Validation 


```{r}

set.seed(123)

data_split <- initial_split(data, prop = .7, strata = BodyTemp)

train_data <- training(data_split)
test_data  <- testing(data_split)

folds <- vfold_cv(train_data, v = 5, r=5, strata= "BodyTemp")
folds

```


Creating a Recipe/Workflow
```{r}
#Recipe() has two arguments: a formula and the data
bodytemp_cont_rec <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) #adding step_dummy

#Build a model specification using the parsnip package
lm_mod <- linear_reg() %>%
  set_engine("lm") 

#Model workflow pairs a model and recipe together
bodytemp_cont_workflow <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(bodytemp_cont_rec)


```

#Fitting the Null Model
```{r}

# Creates a simple recipe that fits null model
bodytmp_rec_null <- recipe(BodyTemp ~  1 , data = train_data)

# Set a model as we did in the previous exercise
lr_mod <- 
  linear_reg() %>% 
  set_engine("lm")

bodytmp_wflow_null <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(bodytmp_rec_null)

# Fitting the model
bodytmp_fit_null <- 
  bodytmp_wflow_null %>% 
  fit(data = train_data)

# Extracting Model/Recipes with Parsnip
bodytmp_fit_null %>% 
  extract_fit_parsnip() %>% 
  tidy()


# Obtaining Predictions
predict(bodytmp_fit_null, train_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, train_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_train <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.21
rmse_train



# Now on Test Data


# Obtaining Predictions
predict(bodytmp_fit_null, test_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, test_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_test <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.16
rmse_test 
```

###Here we see that the RMSE on train data from the null model is 1.21
```{r}
# RMSE 1.21
rmse_train
```



#Fitting a Tree
```{r}
# TREE 

# model specification
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")    # setting it to regression instead of classification

tune_spec


# tuning grid specification
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)


tree_grid %>% 
  count(tree_depth)


# Tune a workflow() that bundles together a model
# specification and a recipe or model preprocessor.
# Here we use a workflow() with a straightforward formula; 
# if this model required more involved data preprocessing, 
# we could use add_recipe() instead of add_formula().


tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(bodytemp_cont_rec) # using predefined recipe

```



```{r}
# tuning using cross-validation and the tune_grid() function
tree_res <- 
  tree_wf %>% 
  tune_grid(resamples = folds, grid = tree_grid)
```

```{r}
tree_res %>% 
  collect_metrics()

# Once you have done the tuning, you can take a look at some diagnostics
#by sending your object returned from the tune_grid() function to autoplot(). 
#For instance if you tuned the tree and saved the result as tree_tune_res,
#you can run tree_tune_res %>% autoplot(). Depending on the model, the plot
#will be different, but in general it shows you what happened during the tuning process.

#plotting metrics
tree_res %>% autoplot()


# Next, you want to get the model that the tuning process has determined 
# is the best. You can get the best-fit model with select_best() 
# and finalize_workflow() and then do one more fit to the training data with 
# this final workflow using the fit() function. Follow the examples in the tutorial.

# selecting best
best_tree <- tree_res %>%
  select_best(tree_res, metric = "rsq")

best_tree



# finalizing model
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf

# one more fit to the training data with 
# this final workflow using the fit() function

final_fit <- 
  final_wf %>%
  last_fit(data_split) 


# RMSE= 1.23, not much different from the null
final_fit %>%
  collect_metrics()

#Collecting Predictions
tree_pred <- final_fit %>%
  collect_predictions() 

```




```{r}
# Make two plots, one that shows model predictions from the tuned model 
# versus actual outcomes
  
ggplot(data=tree_pred, aes(x=.pred, y=BodyTemp)) + geom_point() + labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Actual Outcomes") 

#calculating residuals
tree_pred$resid <- tree_pred$BodyTemp - tree_pred$.pred 

# one that plots residuals.
# plotting residuals 
ggplot(data=tree_pred, aes(x=.pred , y=resid)) + geom_point() +
  labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Residuals") 



# Look at/print the model performance and compare it with the null model
# (still only on training data). Here, we want the performance of the tuned, 
# best-fitting model on the CV dataset (we are not yet touching the test data). 
# You can get that for instance with the show_best() function, which gives you
# the mean cross-validated performance for the best models. It also shows the
# standard deviation for the performance. Compare that model performance with the null model

```


###Comparing RMSE to Null

The tree model does not perform very well, and the model only predicts a few discrete
outcome values. That’s also noticeable when we compare RMSE for the tree model(1.23) 
and the null model (1.21). They are very similar.

```{r}
# RMSE= 1.23
show_best(final_fit, metric= "rmse")

# Null Model. RMSE 1.21
rmse_train
```


###Tree Plot
```{r}
final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```


### Estimate variable importance based on the model’s structure.
```{r}
library(vip)

final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()
```



## LASSO linear model

Repeating the steps above, now for LASSO.


### LASSO setup

```{r, start-lasso}
#model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) #mixture = 1 means LASSO model
#workflow
lasso_wf <- workflow() %>%
  add_model(lasso_model) %>% 
  add_recipe(bodytemp_cont_rec)
```

### LASSO tuning

```{r, tune-lasso}

#tuning grid
lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))
#tune model
lasso_tune_res <- lasso_wf %>% 
  tune_grid(resamples = folds,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
            )

```


### LASSO evaluation

```{r}
#see a plot of performance for different tuning parameters
lasso_tune_res %>% autoplot()
```




```{r}
# get the tuned model that performs best 
best_lasso <- lasso_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_lasso_wf <- lasso_wf %>% finalize_workflow(best_lasso)
# fitting best performing model
best_lasso_fit <- best_lasso_wf %>% 
  fit(data = train_data)
lasso_pred <- predict(best_lasso_fit, train_data)
```

Plotting LASSO variables as function of tuning parameter


```{r}
x <- best_lasso_fit$fit$fit$fit
plot(x, "lambda")
```

This shows the variables that are part of the best-fit LASSO model, i.e. those that have a non-zero coefficient.

```{r}
tidy(extract_fit_parsnip(best_lasso_fit)) %>% filter(estimate != 0)
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(lasso_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(lasso_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```


Looking at model performance. 

```{r}
# code borrowed from Zane :)
lasso_perfomance <- lasso_tune_res %>% show_best(n = 1)
print(lasso_perfomance)
```

So seems that the LASSO model is not quite as bad as the tree, but it is still not a good model.


## Random forest model

Repeating the steps above, now for a random forest.


### Random forest setup


```{r, start-rf}
rf_model <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")           
```


```{r}
#workflow
rf_wf <- workflow() %>%
  add_model(rf_model) %>% 
  add_recipe(bodytemp_cont_rec)
```


### Random forest tuning

```{r, tune-rf}
#parallel computing
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
#tuning grid
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )
# tune the model, optimizing RMSE
rf_tune_res <- rf_wf %>%
  tune_grid(
            resamples = folds, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(rmse) 
  )
# turn off parallel cluster
stopCluster(cl)
```


### Random forest evaluation

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res %>% autoplot()
```

```{r}
# get the tuned model that performs best 
best_rf <- rf_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_rf_wf <- rf_wf %>% finalize_workflow(best_rf)
# fitting best performing model
best_rf_fit <- best_rf_wf %>% 
  fit(data = train_data)
rf_pred <- predict(best_rf_fit, train_data)
```


For random forest models, one can't easily look at the final model. One can however look at the most important predictors for the final model.

```{r}
#pull out the fit object
x <- best_rf_fit$fit$fit$fit
#plot variable importance
vip::vip(x, num_features = 20)
```





Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(rf_pred$.pred,train_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(rf_pred$.pred-train_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```


Looking at model performance. 

```{r}
# code borrowed from Zane :)
rf_perfomance <- rf_tune_res %>% show_best(n = 1)
print(rf_perfomance)
```


# Model Selection

Here we go with this LASSO, even though most models do not perform well.

# Final Model Evaluation

```{r}
# for reasons that make no sense (likely a bug in tidymodels)
# I need to re-start a parallel cluster here to get the command below to work 
cl <- makePSOCKcluster(ncores)
registerDoParallel(cl)
# fit on the training set and evaluate on test set
final_fit <- best_lasso_wf  %>% last_fit(data_split)
stopCluster(cl)
```

Let's look at the performance of the final fit

```{r}
test_performance <- final_fit %>% collect_metrics()
print(test_performance)
```

If we compare the RMSE to the performance on the training data, we see it's similar. That's good. If we compare it to the performance of the null model on the test data, we see it's not much better. That's bad. It shows again that none of these models are good.

And just another look at the diagnostic plots for the test data.

```{r}
test_predictions <- final_fit %>% collect_predictions()
```


Plotting observed/predicted and residuals.

```{r}
#predicted versus observed
plot(test_predictions$.pred,test_data$BodyTemp, xlim =c(97,103), ylim=c(97,103))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
#residuals
plot(test_predictions$.pred-test_data$BodyTemp)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

Still bad.


# Overall conclusion

It appears that none of the models fit the data well.


---
title: "Exercise 11"
author: "Nicholas Mallis"
date: "11/2/2021"
output: html_document
---
Note: The contents of this exercise include code, output, and figures for fitting the null models and Decision Tree. I unfortunately could not get my LASSO or Random Forest models to work. Everytime I try to run tune_grid(), R crashes. I tried multiple ways and seeked help, but nothing would work.I'm not sure if it's my computer. I have a seperate R script called LASSO and RF.R which includes include code, output, and figures for fitting the null models, LASSO, and Random Forests. While these do not work on my computer, maybe they will on yours?

###Importing and Processing Data
```{r message=FALSE, results =FALSE}
#loading packages
library(here) #for data loading/saving
library(tidyverse)
library(recipes)
library(tidymodels)
library(workflowr) 
library(parsnip)
library(rsample)
library(rpart)
library(glmnet)
library(ranger)
library(modeldata)
library(rpart.plot)

#first loading in processed data
data_location <- here::here("data","processed_data","processeddata.rds")

#load data. 
data <- readRDS(data_location)

#checking
glimpse(data)


# For those symptoms where you have both multiple levels and yes/no, 
# remove all the yes/no versions. That should remove 4 variables.


data <- select(data, -c(WeaknessYN, CoughYN, CoughYN2, MyalgiaYN))

glimpse(data)

# We also want to code the 3 ordinal/multi-level factors as ordered, 
# so make sure they are coded as ordered factors. 
# The order should of course be None/Mild/Moderate/Severe.

#as.factor(data$Weakness)
#table(data$Weakness)

#as.factor(data$CoughIntensity)
#table(data$CoughIntensity)

#as.factor(data$Myalgia)
#table(data$Myalgia)


data <- mutate(data, Weakness = factor(Weakness, levels = c("None", "Mild",
                                                            "Moderate","Severe"),ordered = TRUE))
data <- mutate(data, CoughIntensity = factor(CoughIntensity, levels = c("None", "Mild",
                                                                        "Moderate","Severe"),ordered = TRUE))
data <- mutate(data, Myalgia = factor(Myalgia, levels = c("None", "Mild",
                                                          "Moderate","Severe"),ordered = TRUE))

# But it’s often better to decide manually for each variable based on your scientific 
# expertise if you want to remove it or not. We’ll take that approach here.
# After looking at the data, we decide to remove those binary predictors that 
# have <50 entries in one category (there are 2). Write code to remove them.

library(table1) #loading the table 1 package

table1 <- table1(~ . , data=data, overall="Total")
table1

# Looks like hearing and vision both don't have more than 50 yes
data <- select(data, -c(Hearing, Vision))

```

###Taking a glimpse at new processed data
```{r}
glimpse(data)
```

###Setting up: Random Seed, Data Split, Cross Validation and Recipe/Workflow
```{r}
# Start by setting the random seed to 123.
# This should make everything reproducible and
# everyone should get the same results.

set.seed(123)


# Split the dataset into 70% training, 30% testing. 
# Also use the outcome BodyTemp as stratification. 
# This allows for more balanced outcome values in the train and test sets.
# See e.g., section 3 of the Get Started tutorial.


data_split <- initial_split(data, prop = .7, strata = BodyTemp)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)


# We want to do 5-fold cross-validation, 5 times repeated.
# (There’s no specific reason to do this 5x5 pattern, other 
# than to show you that there are different ways to pick the sample,
# and that I want you to not use the default.) For the CV folds, 
# we also want to stratify on BodyTemp, as we did for the main train/test split.
# Use the vfold_cv function to create a resample object for the training data 
# with these specifications.

folds <- vfold_cv(train_data, v = 5, r=5, strata= "BodyTemp")
folds


# Create a recipe for the data and fitting. You won’t need to do much, 
# just make sure you code the categorical variables as dummy variables,
# otherwise things might not work smoothly. For that, you want to use 
# the step_dummy function and pick all nominal predictor variables 
# (which are actually all variables here, since the only continuous
# variable is our outcome).


#Recipe() has two arguments: a formula and the data
bodytemp_cont_rec <- recipe(BodyTemp ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors()) #adding step_dummy

#Build a model specification using the parsnip package
lm_mod <- linear_reg() %>%
  set_engine("lm") 

#Model workflow pairs a model and recipe together
bodytemp_cont_workflow <- 
  workflow() %>%
  add_model(lm_mod) %>%
  add_recipe(bodytemp_cont_rec)


```

###Fitting the Null Model
```{r}
# Write some code to compute the performance of a null model, i.e. 
# a “model” that doesn’t use any predictor information. For a 
# continuous outcome and RMSE as our metric, a null model is one 
# that always predicts the mean of the outcome. Compute the RMSE 
# for both training and test data for such a “model”. We’ll use that 
# later to compare it to the performance of our real models. 
# Of course, we expect/hope our real models that use predictor 
# information to be better. If they aren’t that means they are no good.


# Creates a simple recipe that fits null model
bodytmp_rec_null <- recipe(BodyTemp ~  1 , data = train_data)

# Set a model as we did in the previous exercise
lr_mod <- 
  linear_reg() %>% 
  set_engine("lm")


# Use the workflow() package to create a
# simple workflow that fits a linear model
# to all predictors using the glm function
bodytmp_wflow_null <- 
  workflow() %>% 
  add_model(lr_mod) %>% 
  add_recipe(bodytmp_rec_null)

# Fitting the model
bodytmp_fit_null <- 
  bodytmp_wflow_null %>% 
  fit(data = train_data)

# Extracting Model/Recipes with Parsnip
bodytmp_fit_null %>% 
  extract_fit_parsnip() %>% 
  tidy()





# Obtaining Predictions
predict(bodytmp_fit_null, train_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, train_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_train <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.21
rmse_train



# Now on Test Data


# Obtaining Predictions
predict(bodytmp_fit_null, test_data)

bodytmp_aug_null <- 
  augment(bodytmp_fit_null, test_data)

bodytmp_aug_null %>%
  select(BodyTemp)


# Calculating Root RMSE 
rmse_test <- bodytmp_aug_null %>% 
  rmse(truth = BodyTemp, .pred)

# RMSE 1.16
rmse_test 
```

###Here we see that the RMSE on train data from the null model is 1.21
```{r}
# RMSE 1.21
rmse_train
```



#Fitting a Tree
```{r}
# TREE 

# model specification
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")    # setting it to regression instead of classification

tune_spec


# tuning grid specification
tree_grid <- grid_regular(cost_complexity(),
                          tree_depth(),
                          levels = 5)


tree_grid %>% 
  count(tree_depth)


# Tune a workflow() that bundles together a model
# specification and a recipe or model preprocessor.
# Here we use a workflow() with a straightforward formula; 
# if this model required more involved data preprocessing, 
# we could use add_recipe() instead of add_formula().


tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_recipe(bodytemp_cont_rec) # using predefined recipe

```



```{r}
# tuning using cross-validation and the tune_grid() function
tree_res <- 
  tree_wf %>% 
  tune_grid(resamples = folds, grid = tree_grid)
```

```{r}
tree_res %>% 
  collect_metrics()


# Once you have done the tuning, you can take a look at some diagnostics
#by sending your object returned from the tune_grid() function to autoplot(). 
#For instance if you tuned the tree and saved the result as tree_tune_res,
#you can run tree_tune_res %>% autoplot(). Depending on the model, the plot
#will be different, but in general it shows you what happened during the tuning process.

#plotting metrics
tree_res %>% autoplot()



# Plotting Metrics again (looks a bit nicer)
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(size = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)


# Next, you want to get the model that the tuning process has determined 
# is the best. You can get the best-fit model with select_best() 
# and finalize_workflow() and then do one more fit to the training data with 
# this final workflow using the fit() function. Follow the examples in the tutorial.

# selecting best
best_tree <- tree_res %>%
  select_best(tree_res, metric = "rsq")

best_tree



# finalizing model
final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf

# one more fit to the training data with 
# this final workflow using the fit() function

final_fit <- 
  final_wf %>%
  last_fit(data_split) 


# RMSE= 1.23, not much different from the null
final_fit %>%
  collect_metrics()

#Collecting Predictions
tree_pred <- final_fit %>%
  collect_predictions() 



# Make two plots, one that shows model predictions from the tuned model 
# versus actual outcomes
  
ggplot(data=tree_pred, aes(x=.pred, y=BodyTemp)) + geom_point() + labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Actual Outcomes") 

#calculating residuals
tree_pred$resid <- tree_pred$BodyTemp - tree_pred$.pred 

# one that plots residuals.
# plotting residuals 
ggplot(data=tree_pred, aes(x=.pred , y=resid)) + geom_point() +
  labs(title= "Plot of Model Predictions from Tuned Model vs Actual Outcomes", 
       x= "Model Predictions", y= "Residuals") 



# Look at/print the model performance and compare it with the null model
# (still only on training data). Here, we want the performance of the tuned, 
# best-fitting model on the CV dataset (we are not yet touching the test data). 
# You can get that for instance with the show_best() function, which gives you
# the mean cross-validated performance for the best models. It also shows the
# standard deviation for the performance. Compare that model performance with the null model

```

###Comparing RMSE to Null
The tree model does not perform very well, and the model only predicts a few discrete
outcome values. That’s also noticeable when we compare RMSE for the tree model(1.23) 
and the null model (1.21). They are very similar.

```{r}
# RMSE= 1.23
show_best(final_fit, metric= "rmse")

# Null Model. RMSE 1.21
rmse_train
```


###Tree Plot
```{r}
final_fit %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)

```


### Estimate variable importance based on the model’s structure.
```{r}
library(vip)

final_fit %>% 
  extract_fit_parsnip() %>% 
  vip()
```




###Model selection: Well since I can't seem to figure out how to get the LASSO or RF to run, I'm going with the Decision Tree Model as the best one. 
```{r}

last_fit()

chosen <- 
  final_wf %>%
  last_fit(data_split) 


```
Once you have implemented above steps for the 3 models, you should a “best” fit for each one based on the tuning process. For each best model you should have performance, uncertainty around the performance measure, and some diagnostic plots. While for any real research project, you likely want to look deeper (e.g. at uncertainty in predictions instead of just overall performance), for now this is enough. Pick one of the three models. Explain why you pick it. There is no single answer that’s correct, I just want you to learn to reason/argue for why you are doing something: in this case justify why you are picking the model you do.

Final evaluation
Once you picked your final model, you are allowed to once – and only once – fit it to the test data and check how well it performs on that data. This gives you a somewhat honest estimate of how the model might perform for new, unseen data. You can do that using the last_fit() function applied to the model you end up choosing. For the final model applied to the test set, report performance and the diagnostic plots as above.

And that concludes what is likely a fairly long exercise. The code itself is not that long, but it will take you time to cobble it together from the tidymodel tutorial and possibly other sources.





